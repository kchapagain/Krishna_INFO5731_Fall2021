{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "In_class_exercise_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kchapagain/Krishna_INFO5731_Fall2021/blob/main/In_class_exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcQFD_eWli4D"
      },
      "source": [
        "## The Second In-class-exercise (9/19/2021, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMvr2N9Yli4J"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJToKDULli4L"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "WIzpBKX-li4M",
        "outputId": "480c68c7-54d5-41a5-efb0-a8cd650900f9"
      },
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Question. Describe an interesting research question (or practical question) you have in mind.\n",
        "  The research topic that I would be interested at will be Covid 19.The research question for this topic would be:\n",
        "   1. What state has the highest covid rate?\n",
        "   2. What is the mortality rate of Covid 19 by the state?\n",
        "   3. Which could be the reasons the particular state has such higher cases?\n",
        "   4. What is the trend of Covid 19 from last year to now?\n",
        "   5. Death vs infected by Covid 19\n",
        "\n",
        "Question. What kind of data should be collected to answer the question(s)?\n",
        "  The WHO data, CDC published data can be the major source of Covid 19 data for this analysis since they are accurate and provided as Category of States.\n",
        "  Data can be in any format like tables, json or html. We can perform data mining and get the required data from the sources available.\n",
        "\n",
        "Question. How many data needed for the analysis?\n",
        "  Depends on the accuracy needed for the analysis, higher the data higher will be our accuracy. Since this topic is very critical in nature, I would recommend\n",
        "  not less than 10,000 data at a time for the analysis.\n",
        "\n",
        " Question. The detail steps for collecting and save the data.\n",
        "  The steps can be as simple as importing csv files and tables into python and saving it.\n",
        "  \n",
        "  Another process would be Data Mining.\n",
        "  --> Using libraries like Beautiful Soup to access the url and fetch the data in html format.\n",
        "  --> Filtering the useful data and converting them into dataframes using pandas libraries in python.\n",
        "  --> Saving the data as csv in the assigned path.\n",
        "\n",
        "  Another process can be the use of API.\n",
        "  --> Setting up the API credentials for those websites that needed them\n",
        "  --> Accessing and importing all the data by the help of API in html format.\n",
        "  --> Filtering the useful data by using beautiful soup and other libraries available\n",
        "  --> Converting the useful data in dataframes.\n",
        "  --> Saving the data in csv or other format in the assigned path.\n",
        "\n",
        "'''"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nQuestion. Describe an interesting research question (or practical question) you have in mind.\\n  The research topic that I would be interested at will be Covid 19.The research question for this topic would be:\\n   1. What state has the highest covid rate?\\n   2. What is the mortality rate of Covid 19 by the state?\\n   3. Which could be the reasons the particular state has such higher cases?\\n   4. What is the trend of Covid 19 from last year to now?\\n   5. Death vs infected by Covid 19\\n\\nQuestion. What kind of data should be collected to answer the question(s)?\\n  The WHO data, CDC published data can be the major source of Covid 19 data for this analysis since they are accurate and provided as Category of States.\\n  Data can be in any format like tables, json or html. We can perform data mining and get the required data from the sources available.\\n\\nQuestion. How many data needed for the analysis?\\n  Depends on the accuracy needed for the analysis, higher the data higher will be our accuracy. Since this topic is very critical in nature, I would recommend\\n  not less than 10,000 data at a time for the analysis.\\n\\n Question. The detail steps for collecting and save the data.\\n  The steps can be as simple as importing csv files and tables into python and saving it.\\n  \\n  Another process would be Data Mining.\\n  --> Using libraries like Beautiful Soup to access the url and fetch the data in html format.\\n  --> Filtering the useful data and converting them into dataframes using pandas libraries in python.\\n  --> Saving the data as csv in the assigned path.\\n\\n  Another process can be the use of API.\\n  --> Setting up the API credentials for those websites that needed them\\n  --> Accessing and importing all the data by the help of API in html format.\\n  --> Filtering the useful data by using beautiful soup and other libraries available\\n  --> Converting the useful data in dataframes.\\n  --> Saving the data in csv or other format in the assigned path.\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjyD2i0Uli4N"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 reviews of a movie from IMDB (https://www.imdb.com/) or 1000 reviews of a product from Amazon (https://www.amazon.com/).\n",
        "\n",
        "As for the IMDB movie review, the following informtion need to be collected (for example: https://www.imdb.com/title/tt6751668/reviews?ref_=tt_urv):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time\n",
        "\n",
        "\n",
        "As for the Amazon product review, the following information need to be collected (for example: https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_3?crid=2E3C55VKJX0K3&dchild=1&keywords=machine+learning+andrew+ng&qid=1631718619&sr=8-3):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "BpLJAKpr6TQr",
        "outputId": "6ceb9f76-4c54-4a63-fd12-ea51b9e5bf0c"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import json\n",
        "\n",
        "# Assigning the wesite link to url\n",
        "url =  \"https://www.imdb.com/title/tt6751668/reviews?sort=userRating&dir=desc&ratingFilter=0\"\n",
        "#Opening the url\n",
        "page = urllib.request.urlopen(url)\n",
        "#Changing the format of url to soup\n",
        "soup = BeautifulSoup(page)\n",
        "\n",
        "#Creating empty Lists\n",
        "n = 1000 # Number of Reviews\n",
        "user_name = []\n",
        "star = []\n",
        "review_title = []\n",
        "review_text = []\n",
        "review_posted_time = []\n",
        "\n",
        "#Using for lopp to take out the attributes\n",
        "for u in soup.find_all(\"span\",{'class':'display-name-link'},'a'):\n",
        "    u = user_name.append(u.text.strip())\n",
        "\n",
        "for s in soup.find_all(\"span\",{'class':'rating-other-user-rating'},'span'):\n",
        "    s = star.append(s.text.strip())\n",
        "\n",
        "for r_ti in soup.find_all(\"a\",{'class':'title'}):\n",
        "    r_ti =review_title.append(r_ti.text.strip())\n",
        "\n",
        "for r_te in soup.find_all(\"div\",{'class':'text show-more__control'}):\n",
        "    r_te =review_text.append(r_te.text.strip())\n",
        "\n",
        "for r_t in soup.find_all(\"span\",{'class':'review-date'}):\n",
        "    r_t = review_posted_time.append(r_t.text.strip())\n",
        "\n",
        "#Changing the liststo DataFrame\n",
        "Reviews = pd.DataFrame({'User_Name':user_name, 'Star':star, 'Review_Title':review_title, 'Review_Text':review_text, 'Review_Posted_Time':review_posted_time})\n",
        "Reviews.head(10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_Name</th>\n",
              "      <th>Star</th>\n",
              "      <th>Review_Title</th>\n",
              "      <th>Review_Text</th>\n",
              "      <th>Review_Posted_Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>khizernawaz</td>\n",
              "      <td>10/10</td>\n",
              "      <td>What a great masterpiece movie</td>\n",
              "      <td>Fantastic gtreatest movie. Win the best motion...</td>\n",
              "      <td>6 February 2020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>khizernawaz</td>\n",
              "      <td>10/10</td>\n",
              "      <td>What a great movie, bravo</td>\n",
              "      <td>What a great movie, bravo.. bong direct this m...</td>\n",
              "      <td>14 August 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>deliaandreea-41214</td>\n",
              "      <td>10/10</td>\n",
              "      <td>No words left</td>\n",
              "      <td>Literally, the best movie of the decade. And n...</td>\n",
              "      <td>14 February 2020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pretentious_Viewer</td>\n",
              "      <td>10/10</td>\n",
              "      <td>One of the few films that can be granted the t...</td>\n",
              "      <td>That's it. That's all I'm writing about this f...</td>\n",
              "      <td>20 December 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>andresatyadharmaa</td>\n",
              "      <td>10/10</td>\n",
              "      <td>Really good</td>\n",
              "      <td>Good movie to watch when u are boring\\nReally ...</td>\n",
              "      <td>14 August 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>nintendoisdabomb</td>\n",
              "      <td>10/10</td>\n",
              "      <td>Scariest movie I've experienced.</td>\n",
              "      <td>This movie had me terrified the entire time wh...</td>\n",
              "      <td>26 March 2020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>sac-09612</td>\n",
              "      <td>10/10</td>\n",
              "      <td>A Grifter Great</td>\n",
              "      <td>When you write a review this late into a movie...</td>\n",
              "      <td>17 February 2020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>asherwiegartz</td>\n",
              "      <td>10/10</td>\n",
              "      <td>A thrilling and hilarious masterpiece.</td>\n",
              "      <td>Man, where do I even begin? Let's start with t...</td>\n",
              "      <td>10 October 2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>nuuratbinici</td>\n",
              "      <td>10/10</td>\n",
              "      <td>Well deserved an Oscar</td>\n",
              "      <td>It is pulling you into reality and at the same...</td>\n",
              "      <td>27 November 2020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>VishalLodaya</td>\n",
              "      <td>10/10</td>\n",
              "      <td>It is a class-conscious thriller that you woul...</td>\n",
              "      <td>Saw Parasite yesterday. Something that we miss...</td>\n",
              "      <td>8 May 2020</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            User_Name  ... Review_Posted_Time\n",
              "0         khizernawaz  ...    6 February 2020\n",
              "1         khizernawaz  ...     14 August 2019\n",
              "2  deliaandreea-41214  ...   14 February 2020\n",
              "3  Pretentious_Viewer  ...   20 December 2019\n",
              "4   andresatyadharmaa  ...     14 August 2019\n",
              "5    nintendoisdabomb  ...      26 March 2020\n",
              "6           sac-09612  ...   17 February 2020\n",
              "7       asherwiegartz  ...    10 October 2019\n",
              "8        nuuratbinici  ...   27 November 2020\n",
              "9        VishalLodaya  ...         8 May 2020\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoMShcx5li4P"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/). \n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "-d1IxgvN6aYm",
        "outputId": "63d16513-146f-4a8a-e0bb-0b8b528d5fe9"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import json\n",
        "\n",
        "# Assigning the wesite link to url\n",
        "url =  \"https://citeseerx.ist.psu.edu/search?q=data+science&submit.x=0&submit.y=0&sort=rlv&t=doc\"\n",
        "#Opening the url\n",
        "page = urllib.request.urlopen(url)\n",
        "#Changing the format of url to soup\n",
        "soup = BeautifulSoup(page)\n",
        "\n",
        "title = []\n",
        "year = []\n",
        "authors = []\n",
        "abstract = []\n",
        "cited = []  # My data doesnot have venue so replacing it with cited\n",
        "\n",
        "#soup.find_all('a',{'class':'remove doc_details'})\n",
        "\n",
        "for t in soup.find_all('a',{'class':'remove doc_details'}):\n",
        "    t=title.append(t.text.strip())\n",
        "\n",
        "for y in soup.find_all('span',{'class':'pubyear'}):\n",
        "    y=year.append(y.text.strip())\n",
        "\n",
        "for a in soup.find_all('span',{'class':'authors'}):\n",
        "    a=authors.append(a.text.strip())\n",
        "\n",
        "for ab in soup.find_all('div',{'class':'snippet'}):\n",
        "    ab=abstract.append(ab.text.strip())\n",
        "\n",
        "for c in soup.find_all('a',{'class':'citation remove'}):\n",
        "    c=cited.append(c.text.strip())\n",
        "\n",
        "Articles = pd.DataFrame({'Title':title, 'Year':year, 'Authors':authors, 'Abstract':abstract, 'Cited':cited})\n",
        "Articles['Authors'] = Articles['Authors'].str.replace('by \\n ','')\n",
        "Articles['Year'] = Articles['Year'].str.replace(',','')\n",
        "Articles['Abstract'] = Articles['Abstract'].str.replace('\"...','' )\n",
        "Articles.head(10)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Year</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Cited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Data Streams: Algorithms and Applications</td>\n",
              "      <td>2005</td>\n",
              "      <td>S. Muthukrishnan</td>\n",
              "      <td>In the data stream scenario, input arrives ve...</td>\n",
              "      <td>Cited by 533 (22 self)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bayesian Data Analysis</td>\n",
              "      <td>1995</td>\n",
              "      <td>Andrew Gelman, Christian Robe...</td>\n",
              "      <td>I actually own a copy of Harold Jeffreys’s Th...</td>\n",
              "      <td>Cited by 2194 (63 self)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Voronoi diagrams -- a survey of a fundamental ...</td>\n",
              "      <td>1991</td>\n",
              "      <td>Franz Aurenhammer</td>\n",
              "      <td>This paper presents a survey of the Voronoi d...</td>\n",
              "      <td>Cited by 743 (5 self)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From Data Mining to Knowledge Discovery in Dat...</td>\n",
              "      <td>1996</td>\n",
              "      <td>Usama Fayyad , Gregory Piatet...</td>\n",
              "      <td>■ Data mining and knowledge discovery in dat...</td>\n",
              "      <td>Cited by 538 (0 self)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Survey of clustering algorithms</td>\n",
              "      <td>2005</td>\n",
              "      <td>Rui Xu, Donald Wunsch II</td>\n",
              "      <td>Data analysis plays an indispensable role for...</td>\n",
              "      <td>Cited by 499 (4 self)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Thresholding of statistical maps in functional...</td>\n",
              "      <td>2002</td>\n",
              "      <td>Christopher R Genovese , Nico...</td>\n",
              "      <td>Finding objective and effective thresholds f...</td>\n",
              "      <td>Cited by 521 (9 self)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Maintaining knowledge about temporal intervals</td>\n",
              "      <td>1983</td>\n",
              "      <td>James F. Allen</td>\n",
              "      <td>The problem of representing temporal knowledg...</td>\n",
              "      <td>Cited by 2942 (13 self)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Status quo bias in decision making</td>\n",
              "      <td>1988</td>\n",
              "      <td>William Samuelson, Richard Ze...</td>\n",
              "      <td>economics, rationality Most real decisions, u...</td>\n",
              "      <td>Cited by 641 (21 self)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Rough Sets.</td>\n",
              "      <td>1982</td>\n",
              "      <td>Zdzis Law Pawlak , George All...</td>\n",
              "      <td>Abstract. This article presents some general...</td>\n",
              "      <td>Cited by 793 (13 self)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Modeling TCP Throughput: A Simple Model and it...</td>\n",
              "      <td>1998</td>\n",
              "      <td>Jitendra Padhye, Victor Firoi...</td>\n",
              "      <td>In this paper we develop a simple analytic ch...</td>\n",
              "      <td>Cited by 1337 (36 self)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Title  ...                    Cited\n",
              "0          Data Streams: Algorithms and Applications  ...   Cited by 533 (22 self)\n",
              "1                             Bayesian Data Analysis  ...  Cited by 2194 (63 self)\n",
              "2  Voronoi diagrams -- a survey of a fundamental ...  ...    Cited by 743 (5 self)\n",
              "3  From Data Mining to Knowledge Discovery in Dat...  ...    Cited by 538 (0 self)\n",
              "4                    Survey of clustering algorithms  ...    Cited by 499 (4 self)\n",
              "5  Thresholding of statistical maps in functional...  ...    Cited by 521 (9 self)\n",
              "6     Maintaining knowledge about temporal intervals  ...  Cited by 2942 (13 self)\n",
              "7                 Status quo bias in decision making  ...   Cited by 641 (21 self)\n",
              "8                                        Rough Sets.  ...   Cited by 793 (13 self)\n",
              "9  Modeling TCP Throughput: A Simple Model and it...  ...  Cited by 1337 (36 self)\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFowabSYli4R"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "COjvOJHB6kEZ",
        "outputId": "a9616699-7aa2-412f-e3d1-ec9d027582e5"
      },
      "source": [
        "#Importing all the required Libraries\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "text_query = 'Coronavirus'\n",
        "max_tweets = 1000\n",
        "\n",
        "#Getting the keys and secret from developer account\n",
        "consumer_key = \"Sl9QlYe2uIYFB4c56ht3suQpx\"\n",
        "consumer_secret = \"itmSoGGJBHAjnhcy0jZOyBHEEB9oYztj5mLBAa7yJm4tRAb2ai\"\n",
        "access_token = \"1438705493510344707-C3jtWFSrEjfyj0dtBY09lXUVsDL0oi\"\n",
        "access_token_secret = \"XEnNE1Mn6WupA0XnSV8aVNYXrV1k9no4iq6BMDZEOAV5d\"\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "\n",
        "# this will bring 1000 tweets using the set up API\n",
        "tweets = tweepy.Cursor(api.search,q=text_query,lang='en').items(max_tweets)\n",
        " \n",
        "# This will create a nested list of all the attributes in the tweets\n",
        "tweets_list = [[tweet.text, tweet.created_at, tweet.id_str, tweet.user.name, tweet.user.screen_name, tweet.user.id_str, tweet.user.location, tweet.user.url, tweet.user.description, tweet.user.verified, tweet.user.followers_count, tweet.user.friends_count, tweet.user.favourites_count, tweet.user.statuses_count, tweet.user.listed_count, tweet.user.created_at, tweet.user.profile_image_url_https, tweet.user.default_profile, tweet.user.default_profile_image] for tweet in tweets]\n",
        " \n",
        "# Creation of dataframe from tweets_list\n",
        "tweets_df = pd.DataFrame(tweets_list)\n",
        "tweets_df\n",
        "\n",
        "#Arranging the DataFrame as per the Requirement\n",
        "Tweets = pd.DataFrame()\n",
        "Tweets['User_Name'] = tweets_df[4]\n",
        "Tweets['Posted_Time'] = tweets_df[1]\n",
        "Tweets['Text'] = tweets_df[0]\n",
        "Tweets"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_Name</th>\n",
              "      <th>Posted_Time</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SkilledWoundCar</td>\n",
              "      <td>2021-09-20 02:00:28</td>\n",
              "      <td>Thank you yo all of the healthcare professiona...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NILESHg48141360</td>\n",
              "      <td>2021-09-20 02:00:27</td>\n",
              "      <td>RT @BloombergQuint: India reported 30,773 new ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>IslandCoupons</td>\n",
              "      <td>2021-09-20 02:00:26</td>\n",
              "      <td>HAWAII AIRPORTS CORONAVIRUS (COVID-19) UPDATES...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MauiAdventures</td>\n",
              "      <td>2021-09-20 02:00:26</td>\n",
              "      <td>HAWAII AIRPORTS CORONAVIRUS (COVID-19) UPDATES...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>twittdeal</td>\n",
              "      <td>2021-09-20 02:00:26</td>\n",
              "      <td>HAWAII AIRPORTS CORONAVIRUS (COVID-19) UPDATES...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>Chuygzz77</td>\n",
              "      <td>2021-09-20 01:37:14</td>\n",
              "      <td>RT @RealSaavedra: CNN's Dr. Sanjay Gupta on th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>jCPub19</td>\n",
              "      <td>2021-09-20 01:37:14</td>\n",
              "      <td>RT @MonicaGandhi9: Test to stay to avoid quara...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>iWeller_health</td>\n",
              "      <td>2021-09-20 01:37:11</td>\n",
              "      <td>Researcher explores the genetic interplay betw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>KurtSchingoethe</td>\n",
              "      <td>2021-09-20 01:37:11</td>\n",
              "      <td>RT @jsolomonReports: Comedian, actor Chris Roc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>RockyCamp</td>\n",
              "      <td>2021-09-20 01:37:10</td>\n",
              "      <td>RT @freddyatton: COVIDIOT OBITUARY\\n— Jimmy De...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           User_Name  ...                                               Text\n",
              "0    SkilledWoundCar  ...  Thank you yo all of the healthcare professiona...\n",
              "1    NILESHg48141360  ...  RT @BloombergQuint: India reported 30,773 new ...\n",
              "2      IslandCoupons  ...  HAWAII AIRPORTS CORONAVIRUS (COVID-19) UPDATES...\n",
              "3     MauiAdventures  ...  HAWAII AIRPORTS CORONAVIRUS (COVID-19) UPDATES...\n",
              "4          twittdeal  ...  HAWAII AIRPORTS CORONAVIRUS (COVID-19) UPDATES...\n",
              "..               ...  ...                                                ...\n",
              "995        Chuygzz77  ...  RT @RealSaavedra: CNN's Dr. Sanjay Gupta on th...\n",
              "996          jCPub19  ...  RT @MonicaGandhi9: Test to stay to avoid quara...\n",
              "997   iWeller_health  ...  Researcher explores the genetic interplay betw...\n",
              "998  KurtSchingoethe  ...  RT @jsolomonReports: Comedian, actor Chris Roc...\n",
              "999        RockyCamp  ...  RT @freddyatton: COVIDIOT OBITUARY\\n— Jimmy De...\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}